library(caret)
library(e1071)
library(mlbench)
library(Metrics)
library(MLmetrics)

min_max_norm <- function(x) {
  (x - min(x)) / (max(x) - min(x))
}

#read data
train <- read.csv("./data/train_salary.csv", encoding = "UTF-8")
test <- read.csv("./data/test_salary.csv", encoding = "UTF-8")

#select feature
#train <- train[, -c(1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26)]
#test <- test[, -c(1, 3, 4, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26)]
select_feat =  c("company" ,"title","tag" ,"basesalary" , "gender" ,"Race","Education" )
drop_feat = setdiff(colnames(train),select_feat)

#test target:base salary
# train <- subset(train, select = select_feat)
# test <- test$basesalary

# drop the feature
#train <- train[, -c(1, 3, 5, 11,13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24)]
# test <- test[, -c(1, 3, 5, 11,13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24)]

train <- subset(train, select = select_feat)
test <- subset(test, select = select_feat)


#drop na
train <- na.omit(train)
test <- na.omit(test)

print(any(is.na(train)))
print(any(is.na(test)))

#One hot
#dummy <- dummyVars(" ~ .", data=train)
#train <- data.frame(predict(dummy, newdata = train))

#dmy <- dummyVars(" ~ .", data = test)
#test <- data.frame(predict(dmy, newdata = test))

#Label Encoding
# for (i in c(1,2,5,7,10,11)){
for (i in c("company" ,"title","tag", "gender" ,"Race","Education" )){  
  # print(train[, i])
  train[, i] <- as.numeric(factor(train[, i]))
  test[, i] <- as.numeric(factor(test[, i]))  
}

#test target:base salary
test_X <- subset(test, select = -c(basesalary))
test_Y <- test$basesalary

#model
train_ctrl <- trainControl(method = "cv", number = 5)

tune_grid <- expand.grid(nrounds = 200,
                         max_depth = 10,
                         eta = 0.03,
                         gamma = 0.01,
                         colsample_bytree = 0.75,
                         min_child_weight = 0,
                         subsample = 0.5)

# xgboost train
# fit_rlm_cv <- train(basesalary ~ ., data = train, objective="reg:squarederror", method = 'xgbTree',
                    # trControl = train_ctrl, tuneGrid = tune_grid, tuneLength = 10)

# rf train
#fit_rlm_cv <- train(basesalary ~ . , data = train,method = 'ranger',tuneLength = 10,
#                    trControl = train_ctrl,num.trees = 700,importance = "permutation")

# lm svm train
fit_rlm_cv <- train(basesalary ~ ., data = train,  method = 'lm',  trControl = train_ctrl)

#base salary
names(test_Y) <- "basesalary"

#predict base salary
pred_test <- data.frame(predict(fit_rlm_cv, test_X))
names(pred_test) <- "Pred_basesalary"

#E
test_E <- data.frame((test_Y - pred_test))
names(test_E) <- "E"

#output file
test_output <- data.frame(
  basesalary = test_Y,
  Pred_basesalary = pred_test, 
  E = test_E,
  stringsAsFactors = F
)

write.csv(test_output, file = "Output_use_Test_data.csv", row.names = FALSE)

#feature Importance
importance <- varImp(fit_rlm_cv, scale = FALSE)
print(importance)
plot(importance)

#RMSE
print(paste0("RMSE:", round(rmse(test_output$basesalary,test_output$Pred_basesalary), 2)))
print(paste0("R2_Score:", round(R2_Score(test_output$basesalary,test_output$Pred_basesalary), 2)))

